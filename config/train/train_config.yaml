notes: |- # Notes about your run.
  CloneUs Demo Config. 
train_loss: 
eval_loss: 
sample_output: 
# model_id: mistralai/Mistral-7B-v0.1
# model_id: mistralai/Mistral-7B-Instruct-v0.2
# model_id: NousResearch/Llama-2-7b-hf
# model_id: NousResearch/Llama-2-13b-hf
# model_id: TheBloke/Llama-2-13B-GPTQ
model_id: teknium/OpenHermes-2.5-Mistral-7B
# model_id: TinyLlama/TinyLlama-1.1B-Chat-v1.0
# model_id: NousResearch/Nous-Hermes-2-SOLAR-10.7B
# model_id: ISTA-DASLab/Mixtral-8x7B-Instruct-v0_1-AQLM-2Bit-1x16-hf

model_type: MistralForCausalLM # LlamaForCausalLM, MistralForCausalLM, MixtralForCausalLM
base_tune_type: chat # foundation, instruct, chat
chat_template_format: # chatml or blank for no override
tag_placement: replace_role # tag_only, content_prefix, replace_role

quant_method: bnb4 # bnb4, aqlm, gptq
flashattn_lib: unsloth # huggingface, unsloth
attn_implementation: flash_attention_2 # flash_attention_2, sdpa, eagar

padding_side: right # left, right

# Upper token limit on chunk size. Spillover will be placed in a new group.
chunk_size: 8192
batch_size: 4
gradient_accumulation_steps: 2

use_sft_trainer: false
group_by_length: true

num_epochs: 3
learning_rate: 0.0002 
lr_scheduler: cosine # cosine, linear, constant, constant_with_warmup
custom_scheduler: # warmup_const_cosine # to enable, needs lr_scheduler: linear

dataset:
  name: chunkh # chunkh135
  train_jsonl:
  eval_jsonl:
  # hours of silence before we start a new chat chunk. Can be a list to use multiple cutoffs.
  hours_between_sessions: 4 # [1, 3, 5]
  # Minimum number of messages in a chat chunk. Any chunks smaller will be forcibly regrouped.
  min_session_length: 3
  allow_truncation: false
  chatlog_csv: 'data/discord/YOUR_DISCORD_CHAT_FILE.csv' # created from calling scripts/build.py with your DiscordChatExporter JSON export
  eval_frac: 0.005 # or 'after_bot' to use all messages after the first time your bot sent a message in chat. 

# Separator between author_tag and start of text
tag_sep: "\n" # MUST be "\n" when chatml
# Separator between end of text and the next author_tag
postfix: "</X/>" # NOT USED when using chatml format. For others, "</s>\n" works well.
# For chat_ml format, "{author} ({fname})" works well. 
# For non-chat_ml format, additional markup helps e.g.: "[USER:{author}, NAME:{fname}]" or "[{author}, {fname}]:"
# fstring options: author=displayName, fname=firstName, lauthor=lower(displayName)
author_tag: "{author}" # "{author} ({fname})" -- highly recommend using fname if you know everyone's first name
# Same as default except remove 'assistant' from add_generation_prompt
custom_chat_template: "{% for message in messages %}{{'<|im_start|>' + message['role'] + '\n' + message['content'] + '<|im_end|>' + '\n'}}{% endfor %}{% if add_generation_prompt %}{{ '<|im_start|>' }}{% endif %}"

prompt:
  append_msg: false # true when using prompt with a model that doesn't have a system tag
  name_mapping: # Will be filled during scripts/train.py
  template: |-
    Uncensored Discord server chat.
    Topic: offbeat.
    Participants: {name_mapping}
  # template: |-
  #  Simulate a conversation between members in a discord server. Their name=usernames are {name_mapping}.
  #  Your Task: {task}
  task: #|-
  # Reply as one of the members, mirroring their unique personality and writing style as closely as you can.
lora_r: 16
lora_alpha: 32
lora_dropout: 0.0 # Use 0 for unsloth optimizations to take effect
lora_use_rslora: false # More important for high r (>32) -- https://arxiv.org/abs/2312.03732
lora_use_loftq: false
lora_use_dora: false # significant improvement for r=8, but slower + no unsloth support -- https://arxiv.org/abs/2402.09353
lora_target_linear: true
lora_fan_in_fan_out:
lora_target_modules: # "all-linear"
  - gate_proj
  - down_proj
  - up_proj
  - q_proj
  - v_proj
  - k_proj
  - o_proj
  # - embed_tokens
  # - lm_head

optimizer: paged_adamw_32bit
weight_decay: 0.0 # 0.001
warmup_steps: 0
warmup_ratio: 0.03
max_grad_norm: 0.3
neftune_noise_alpha: #5

gradient_checkpointing: true
bf16: true
fp16: false
tf32: true

special_tokens:
  bos_token: "<|im_start|>"
  eos_token: "<|im_end|>" # "</s>"
  unk_token: "<unk>"
  pad_token: "</s>"

pad_vocab_to:
custom_tokens:

resume_from_checkpoint: 
logging_steps: 0.01
save_steps: 500