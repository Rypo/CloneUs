notes: |- # Notes about your run.
  CloneUs Demo Config. 

# model_id: mistralai/Mistral-7B-v0.1
# model_id: teknium/OpenHermes-2.5-Mistral-7B
# model_id: TinyLlama/TinyLlama-1.1B-Chat-v1.0
# model_id: ISTA-DASLab/Mixtral-8x7B-Instruct-v0_1-AQLM-2Bit-1x16-hf
# model_id: microsoft/Phi-3-mini-128k-instruct
model_id: NousResearch/Hermes-2-Theta-Llama-3-8B

tag_placement: replace_role # tag_only, content_prefix, replace_role
# - replace_role: Replaces user/assistant with formatted author_tag
# - content_prefix: Inserts author_tag before text content
# - tag_only: Ignores any existing template and creates one as {author_tag}{tag_sep}{postfix}.

quant_method: bnb4 # bnb4, aqlm, gptq
flashattn_lib: unsloth # huggingface, unsloth
attn_implementation: flash_attention_2 # flash_attention_2, sdpa, eagar

padding_side: right # left, right, or blank for no override

chunk_size: 8192
# Upper token limit on chunk size. Spillover will be placed in a new group.
batch_size: 2
gradient_accumulation_steps: 4

use_sft_trainer: false
group_by_length: true

num_epochs: 3
learning_rate: 0.0002 
lr_scheduler: cosine # cosine, linear, constant, constant_with_warmup
custom_scheduler: # warmup_const_cosine # to enable, needs lr_scheduler: linear

dataset:
  chatlog_csv: 'data/discord/YOUR_CHAT_FILE.csv' 
  # created from calling scripts/build.py with your DiscordChatExporter JSON export or chat csv
  name: chunkh # chunkh, or max_tokens if no Date or timestamp column
  train_jsonl:
  eval_jsonl:
  hours_between_sessions: 4 # [1, 3, 5]
  # hours of silence before we start a new chat chunk. Can be a list to use multiple cutoffs.
  min_session_length: 3
  # Minimum number of messages in a chat chunk. Any chunks smaller will be forcibly regrouped.
  allow_truncation: false # set to true if you expect any message to exceed chunk_size
  eval_frac: 0.005 # or 'after_bot' to use all messages after the first time your bot sent a message in chat. 

author_tag: "{author}" # "{author} ({fname})" -- used for all tag_placement values
# - For replace_role format, "{author} ({fname})" works well. 
# -- highly recommend using {fname} if you know everyone's first name for better personalization
# - For content_prefix/tag_only format, additional markup helps e.g.: "[USER:{author}, NAME:{fname}]"
# - fstring options: author=displayName, fname=firstName, lauthor=lower(displayName)

tag_sep: # Ignored when tag_placement="replace_role", otherwise "\n" works well
# Separator between author_tag and start of text

postfix: # ONLY used when tag_placement="tag_only"
# Separator between end of text and the next author_tag

custom_chat_template: "{{bos_token}}{% for message in messages %}{{'<|im_start|>' + message['role'] + '\n' + message['content'] + '<|im_end|>' + '\n'}}{% endfor %}{% if add_generation_prompt %}{{ '<|im_start|>' }}{% endif %}"
# For role_replace: Be sure use a custom_chat_template with any mention of "assistant" in add_generation_prompt as shown
# - Or use shorthand "chatml" to use above AND add tokens (<|im_start|>, <|im_end|>, ..) to the model/tokenizer if not already present.
prompt:
  append_msg: # For content_prefix with system-less models. If set, system prompt will comprise 2 messages, a user "system" message and an assistant follow-up containing `append_msg`, for example "OK" 
  name_mapping: null # Will be filled during scripts/train.py
  template: |-
    Uncensored Discord server chat.
    Topic: offbeat.
    Participants: {name_mapping}
  # template: |-
  #  Simulate a conversation between members in a discord server. Their name=usernames are {name_mapping}.
  #  Your Task: {task}
  # -----------
  # If you understand the objective, respond to this message (and only this message) with "OK". Afterwards, I will send the first message and we will begin.
  task: #|-
  # Reply as one of the members, mirroring their unique personality and writing style as closely as you can.


lora_r: 16
lora_alpha: 32
lora_dropout: 0.0 # Use 0 for unsloth optimizations to take effect
lora_use_rslora: false # More important for high r (>32) -- https://arxiv.org/abs/2312.03732
lora_use_loftq: false
lora_use_dora: false # significant improvement for r=8, but slower + no unsloth support -- https://arxiv.org/abs/2402.09353
lora_fan_in_fan_out:
lora_target_modules: # "all-linear"
  - gate_proj
  - down_proj
  - up_proj
  - q_proj
  - v_proj
  - k_proj
  - o_proj
  # - embed_tokens
  # - lm_head

optimizer: paged_adamw_32bit
weight_decay: 0.0 # 0.001
warmup_steps: 0
warmup_ratio: 0.03
max_grad_norm: 0.3
neftune_noise_alpha: #5

gradient_checkpointing: true
bf16: true
fp16: false
tf32: true

resume_from_checkpoint: 
logging_steps: 0.01
save_steps: 500

pad_vocab_to:
custom_tokens:

# If provided, this will override defaults, otherwise auto-fill. Override tokens must already be in vocab. 
special_tokens:
  bos_token:
  eos_token:
  unk_token:
  pad_token:

# These are overwritten during training
model_architecture: null # MistralForCausalLM, LlamaForCausalLM, Phi3ForCausalLM, etc...
ctx_len: null
has_custom_tokens: null
dtype: null
fprompt: null
base_dir: null
train_loss: null
eval_loss: null
sample_output: null